{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "160da2cf",
   "metadata": {},
   "source": [
    "# Instruction 11 - Text Mining - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83058e9",
   "metadata": {},
   "source": [
    "Natural language processing (NLP) is a field that focuses on making natural human language usable by computer programs. NLTK, or Natural Language Toolkit, is a Python package that you can use for NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a35cfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57d1816",
   "metadata": {},
   "source": [
    "## Basic preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c029c694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bakullari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bakullari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bakullari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\bakullari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#for sentence tokenization, word tokenization\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "#for tokenization and punctuation removal \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "#to filter out stop words\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#for stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#for lemmatization\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4') # omw=open multilingual wordnet\n",
    "\n",
    "#to compute frequency of text units\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d0efd",
   "metadata": {},
   "source": [
    "Open the \"sagan.txt\" file and display the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9833c430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look again at that dot. That's here. That's home. That's us. On it everyone you love, everyone you know, everyone you ever heard of, every human being who ever was, lived out their lives. The aggregate of our joy and suffering, thousands of confident religions, ideologies, and economic doctrines, every hunter and forager, every hero and coward, every creator and destroyer of civilization, every king and peasant, every young couple in love, every mother and father, hopeful child, inventor and explorer, every teacher of morals, every corrupt politician, every \"superstar,\" every \"supreme leader,\" every saint and sinner in the history of our species lived there-on a mote of dust suspended in a sunbeam.\n",
      "\n",
      "The Earth is a very small stage in a vast cosmic arena. Think of the endless cruelties visited by the inhabitants of one corner of this pixel on the scarcely distinguishable inhabitants of some other corner, how frequent their misunderstandings, how eager they are to kill one another, how fervent their hatreds. Think of the rivers of blood spilled by all those generals and emperors so that, in glory and triumph, they could become the momentary masters of a fraction of a dot.\n",
      "\n",
      "Our posturings, our imagined self-importance, the delusion that we have some privileged position in the Universe, are challenged by this point of pale light. Our planet is a lonely speck in the great enveloping cosmic dark. In our obscurity, in all this vastness, there is no hint that help will come from elsewhere to save us from ourselves.\n",
      "\n",
      "The Earth is the only world known so far to harbor life. There is nowhere else, at least in the near future, to which our species could migrate. Visit, yes. Settle, not yet. Like it or not, for the moment the Earth is where we make our stand.\n",
      "\n",
      "It has been said that astronomy is a humbling and character-building experience. There is perhaps no better demonstration of the folly of human conceits than this distant image of our tiny world. To me, it underscores our responsibility to deal more kindly with one another, and to preserve and cherish the pale blue dot, the only home we've ever known.\n"
     ]
    }
   ],
   "source": [
    "f = open(\"sagan.txt\")\n",
    "text = f.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238d69ce",
   "metadata": {},
   "source": [
    "Obtain the list of sentences contained in the text file and save them using variable `sentences`. Display the first five sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81ee3a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Look again at that dot.', \"That's here.\", \"That's home.\", \"That's us.\", 'On it everyone you love, everyone you know, everyone you ever heard of, every human being who ever was, lived out their lives.']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5dc132",
   "metadata": {},
   "source": [
    "Obtain the list of tokens contained in the first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4063b0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look again at that dot.\n",
      "['Look', 'again', 'at', 'that', 'dot', '.']\n"
     ]
    }
   ],
   "source": [
    "s1 = sentences[0]\n",
    "words_s1 = word_tokenize(s1)\n",
    "print(s1)\n",
    "print(words_s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bf1a76",
   "metadata": {},
   "source": [
    "Apply tokenization (into words), stopword and punctuation removal, and stemming to the corpus (here: a set of sentences). As a result, the new preprocessed corpus must contain a list of documents, where each document (here: sentence) is a list of tokens.\n",
    "*Hint: The stop words provided from nltk are all lowercase.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7ec13fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "stop_list = stopwords.words(\"english\")\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "corpus_stem = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokenized = tokenizer.tokenize(sentence)\n",
    "    filtered = [word.lower() for word in tokenized if word.lower() not in stop_list]\n",
    "    stemmed = [stemmer.stem(word) for word in filtered]\n",
    "    if stemmed != []:\n",
    "        corpus_stem.append(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a21c9913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['look', 'dot'], ['home'], ['us'], ['everyon', 'love', 'everyon', 'know', 'everyon', 'ever', 'heard', 'everi', 'human', 'ever', 'live', 'live'], ['aggreg', 'joy', 'suffer', 'thousand', 'confid', 'religion', 'ideolog', 'econom', 'doctrin', 'everi', 'hunter', 'forag', 'everi', 'hero', 'coward', 'everi', 'creator', 'destroy', 'civil', 'everi', 'king', 'peasant', 'everi', 'young', 'coupl', 'love', 'everi', 'mother', 'father', 'hope', 'child', 'inventor', 'explor', 'everi', 'teacher', 'moral', 'everi', 'corrupt', 'politician', 'everi', 'superstar', 'everi', 'suprem', 'leader', 'everi', 'saint', 'sinner', 'histori', 'speci', 'live', 'mote', 'dust', 'suspend', 'sunbeam']]\n"
     ]
    }
   ],
   "source": [
    "print(corpus_stem[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2846bf",
   "metadata": {},
   "source": [
    "Repeat the same task, but now use lemmatization instead of stemming.\n",
    "\n",
    "(WordNet is a lexical database of semantic relations between words in more than 200 languages. WordNet links words into semantic relations including synonyms, hyponyms, and meronyms. More info: https://wordnet.princeton.edu/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9691ba4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "corpus_lemma = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokenized = tokenizer.tokenize(sentence)\n",
    "    filtered = [word.lower() for word in tokenized if word.lower() not in stop_list]\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in filtered]\n",
    "    if lemmatized != []:\n",
    "        corpus_lemma.append(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92073bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['look', 'dot'], ['home'], ['u'], ['everyone', 'love', 'everyone', 'know', 'everyone', 'ever', 'heard', 'every', 'human', 'ever', 'lived', 'life'], ['aggregate', 'joy', 'suffering', 'thousand', 'confident', 'religion', 'ideology', 'economic', 'doctrine', 'every', 'hunter', 'forager', 'every', 'hero', 'coward', 'every', 'creator', 'destroyer', 'civilization', 'every', 'king', 'peasant', 'every', 'young', 'couple', 'love', 'every', 'mother', 'father', 'hopeful', 'child', 'inventor', 'explorer', 'every', 'teacher', 'moral', 'every', 'corrupt', 'politician', 'every', 'superstar', 'every', 'supreme', 'leader', 'every', 'saint', 'sinner', 'history', 'specie', 'lived', 'mote', 'dust', 'suspended', 'sunbeam']]\n"
     ]
    }
   ],
   "source": [
    "print(corpus_lemma[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eea727",
   "metadata": {},
   "source": [
    "Using the new (lemmatized) corpus, show the ten most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "023a8aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('every', 12),\n",
       " ('dot', 3),\n",
       " ('everyone', 3),\n",
       " ('ever', 3),\n",
       " ('earth', 3),\n",
       " ('one', 3),\n",
       " ('home', 2),\n",
       " ('u', 2),\n",
       " ('love', 2),\n",
       " ('human', 2)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for word_list in corpus_lemma:\n",
    "    w = [word for word in word_list]\n",
    "    words.extend(w)\n",
    "\n",
    "fdist = FreqDist(words)\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf51831",
   "metadata": {},
   "source": [
    "### Part-of-Speech (tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a2b869",
   "metadata": {},
   "source": [
    "Part of speech is a grammatical term that deals with the roles words play when you use them together in sentences. Tagging parts of speech, or POS tagging, is the task of labeling the words in your text according to their part of speech.\n",
    "\n",
    "Hereâ€™s how to get a list of tags and their meanings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c416e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\bakullari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc959228",
   "metadata": {},
   "source": [
    "Given the quote below, label the words according to their part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b82897ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagan_quote = \"If you wish to make an apple pie from scratch, you must first invent the universe.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da34180a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\bakullari\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "words = word_tokenize(sagan_quote)\n",
    "pos_tagged = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be733bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('If', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('wish', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('make', 'VB'),\n",
       " ('an', 'DT'),\n",
       " ('apple', 'NN'),\n",
       " ('pie', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('scratch', 'NN'),\n",
       " (',', ','),\n",
       " ('you', 'PRP'),\n",
       " ('must', 'MD'),\n",
       " ('first', 'VB'),\n",
       " ('invent', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('universe', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4795ad05",
   "metadata": {},
   "source": [
    "Let's try to generate some sentences using the lemmatized corpus we obtained previously as our training data. For this task, we will use the language modeling module of nltk (detailed description: https://www.nltk.org/api/nltk.lm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b0dde4",
   "metadata": {},
   "source": [
    "Build a unigram and a trigram language model using MLE.  For each of the language models, generate a sentence of 7 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eac90033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4736c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the padded_everygram_pipeline goes through the corpus, applies left and right padding to the sentences (adding <s> and </s>)\n",
    "# and obtains the tuples of a given order together with the vocabulary\n",
    "padded_tuples, vocab = padded_everygram_pipeline(1, corpus_lemma)\n",
    "# generate (an empty) ngram language model for some n>0\n",
    "lm_unigram = MLE(1)\n",
    "# generate probabilities (model) given the list of n-grams and the vocabulary\n",
    "lm_unigram.fit(padded_tuples, vocab)\n",
    "\n",
    "padded_tuples, vocab = padded_everygram_pipeline(3, corpus_lemma)\n",
    "lm_trigram = MLE(3)\n",
    "lm_trigram.fit(padded_tuples, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15365cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram sentence\n",
      "['ideology', 'deal', 'triumph', 'ever', 'mote', 'love', 'like']\n",
      "Trigram sentence\n",
      "['</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print('Unigram sentence')\n",
    "print(lm_unigram.generate(7))\n",
    "\n",
    "print('Trigram sentence')\n",
    "print(lm_trigram.generate(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a305261e",
   "metadata": {},
   "source": [
    "## Feature Extraction (Bag of Words, Set of Words, Tfidf, Word2vec, Doc2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b58a76",
   "metadata": {},
   "source": [
    "Feature extraction is about extracting/deriving information from the original features set to create a new features subspace. An example for this from text mining is encoding a piece of text as a numerical vector.\n",
    "\n",
    "*Not to be confused with feature selection! Feature selection is about selecting a subset of features out of the original features in order to reduce model complexity and reduce generalization error introduced due to noise by irrelevant features. An example for this from text mining is the removal of stop words.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b1122b",
   "metadata": {},
   "source": [
    "### Feature extraction method: Bag of Words (using CountVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23c3985",
   "metadata": {},
   "source": [
    "The CountVectorizer provided by sklearn converts a collection of text documents to a matrix of token counts (equivalent to BoW model). Each row is a document and each column correspond to a word in the dictionary. By default, the number of features (dimensions) will be equal to the vocabulary size. The CountVectorizer enables one to apply casefolding, tokenizing and stopword removal at the same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4072f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8128859b",
   "metadata": {},
   "source": [
    "Apply the CountVectorizer to the original corpus (here: `sentences`) with the default parameters. Show the shape of the matrix. Interpret each of the tuple entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ef004eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 204)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = CountVectorizer()\n",
    "count_matrix = bow.fit_transform(sentences)\n",
    "count_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363351fb",
   "metadata": {},
   "source": [
    "There are 20 sentences and 204 extracted features (words in the vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccd08fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 108)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 171)\t1\n",
      "  (0, 41)\t1\n",
      "  (1, 171)\t1\n",
      "  (1, 75)\t1\n",
      "  (2, 171)\t1\n",
      "  (2, 79)\t1\n",
      "  (3, 171)\t1\n",
      "  (3, 185)\t1\n",
      "  (4, 127)\t1\n",
      "  (4, 93)\t1\n",
      "  (4, 53)\t3\n",
      "  (4, 202)\t3\n",
      "  (4, 109)\t1\n",
      "  (4, 98)\t1\n",
      "  (4, 51)\t2\n",
      "  (4, 73)\t1\n",
      "  (4, 126)\t1\n",
      "  (4, 52)\t1\n",
      "  (4, 82)\t1\n",
      "  (4, 11)\t1\n",
      "  (4, 196)\t1\n",
      "  (4, 192)\t1\n",
      "  :\t:\n",
      "  (19, 41)\t1\n",
      "  (19, 79)\t1\n",
      "  (19, 93)\t1\n",
      "  (19, 51)\t1\n",
      "  (19, 172)\t2\n",
      "  (19, 132)\t1\n",
      "  (19, 3)\t2\n",
      "  (19, 128)\t1\n",
      "  (19, 181)\t3\n",
      "  (19, 4)\t1\n",
      "  (19, 193)\t1\n",
      "  (19, 135)\t1\n",
      "  (19, 129)\t1\n",
      "  (19, 99)\t1\n",
      "  (19, 112)\t1\n",
      "  (19, 183)\t1\n",
      "  (19, 147)\t1\n",
      "  (19, 34)\t1\n",
      "  (19, 118)\t1\n",
      "  (19, 96)\t1\n",
      "  (19, 198)\t1\n",
      "  (19, 144)\t1\n",
      "  (19, 19)\t1\n",
      "  (19, 14)\t1\n",
      "  (19, 188)\t1\n"
     ]
    }
   ],
   "source": [
    "print(count_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aea2563",
   "metadata": {},
   "source": [
    "Obtain the vocabulary of your matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5877fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'look': 108,\n",
       " 'again': 0,\n",
       " 'at': 8,\n",
       " 'that': 171,\n",
       " 'dot': 41,\n",
       " 'here': 75,\n",
       " 'home': 79,\n",
       " 'us': 185,\n",
       " 'on': 127,\n",
       " 'it': 93,\n",
       " 'everyone': 53,\n",
       " 'you': 202,\n",
       " 'love': 109,\n",
       " 'know': 98,\n",
       " 'ever': 51,\n",
       " 'heard': 73,\n",
       " 'of': 126,\n",
       " 'every': 52,\n",
       " 'human': 82,\n",
       " 'being': 11,\n",
       " 'who': 196,\n",
       " 'was': 192,\n",
       " 'lived': 105,\n",
       " 'out': 134,\n",
       " 'their': 173,\n",
       " 'lives': 106,\n",
       " 'the': 172,\n",
       " 'aggregate': 1,\n",
       " 'our': 132,\n",
       " 'joy': 94,\n",
       " 'and': 3,\n",
       " 'suffering': 164,\n",
       " 'thousands': 179,\n",
       " 'confident': 24,\n",
       " 'religions': 146,\n",
       " 'ideologies': 85,\n",
       " 'economic': 45,\n",
       " 'doctrines': 40,\n",
       " 'hunter': 84,\n",
       " 'forager': 61,\n",
       " 'hero': 76,\n",
       " 'coward': 30,\n",
       " 'creator': 31,\n",
       " 'destroyer': 37,\n",
       " 'civilization': 21,\n",
       " 'king': 97,\n",
       " 'peasant': 136,\n",
       " 'young': 203,\n",
       " 'couple': 29,\n",
       " 'in': 89,\n",
       " 'mother': 120,\n",
       " 'father': 57,\n",
       " 'hopeful': 80,\n",
       " 'child': 20,\n",
       " 'inventor': 91,\n",
       " 'explorer': 55,\n",
       " 'teacher': 169,\n",
       " 'morals': 117,\n",
       " 'corrupt': 26,\n",
       " 'politician': 141,\n",
       " 'superstar': 166,\n",
       " 'supreme': 167,\n",
       " 'leader': 100,\n",
       " 'saint': 150,\n",
       " 'sinner': 155,\n",
       " 'history': 78,\n",
       " 'species': 159,\n",
       " 'there': 174,\n",
       " 'mote': 119,\n",
       " 'dust': 42,\n",
       " 'suspended': 168,\n",
       " 'sunbeam': 165,\n",
       " 'earth': 44,\n",
       " 'is': 92,\n",
       " 'very': 189,\n",
       " 'small': 156,\n",
       " 'stage': 162,\n",
       " 'vast': 186,\n",
       " 'cosmic': 27,\n",
       " 'arena': 6,\n",
       " 'think': 176,\n",
       " 'endless': 49,\n",
       " 'cruelties': 32,\n",
       " 'visited': 191,\n",
       " 'by': 16,\n",
       " 'inhabitants': 90,\n",
       " 'one': 128,\n",
       " 'corner': 25,\n",
       " 'this': 177,\n",
       " 'pixel': 138,\n",
       " 'scarcely': 152,\n",
       " 'distinguishable': 39,\n",
       " 'some': 158,\n",
       " 'other': 131,\n",
       " 'how': 81,\n",
       " 'frequent': 63,\n",
       " 'misunderstandings': 114,\n",
       " 'eager': 43,\n",
       " 'they': 175,\n",
       " 'are': 5,\n",
       " 'to': 181,\n",
       " 'kill': 95,\n",
       " 'another': 4,\n",
       " 'fervent': 58,\n",
       " 'hatreds': 71,\n",
       " 'rivers': 148,\n",
       " 'blood': 13,\n",
       " 'spilled': 161,\n",
       " 'all': 2,\n",
       " 'those': 178,\n",
       " 'generals': 66,\n",
       " 'emperors': 48,\n",
       " 'so': 157,\n",
       " 'glory': 67,\n",
       " 'triumph': 182,\n",
       " 'could': 28,\n",
       " 'become': 9,\n",
       " 'momentary': 116,\n",
       " 'masters': 111,\n",
       " 'fraction': 62,\n",
       " 'posturings': 143,\n",
       " 'imagined': 87,\n",
       " 'self': 153,\n",
       " 'importance': 88,\n",
       " 'delusion': 35,\n",
       " 'we': 193,\n",
       " 'have': 72,\n",
       " 'privileged': 145,\n",
       " 'position': 142,\n",
       " 'universe': 184,\n",
       " 'challenged': 17,\n",
       " 'point': 140,\n",
       " 'pale': 135,\n",
       " 'light': 103,\n",
       " 'planet': 139,\n",
       " 'lonely': 107,\n",
       " 'speck': 160,\n",
       " 'great': 68,\n",
       " 'enveloping': 50,\n",
       " 'dark': 33,\n",
       " 'obscurity': 125,\n",
       " 'vastness': 187,\n",
       " 'no': 122,\n",
       " 'hint': 77,\n",
       " 'help': 74,\n",
       " 'will': 197,\n",
       " 'come': 22,\n",
       " 'from': 64,\n",
       " 'elsewhere': 47,\n",
       " 'save': 151,\n",
       " 'ourselves': 133,\n",
       " 'only': 129,\n",
       " 'world': 199,\n",
       " 'known': 99,\n",
       " 'far': 56,\n",
       " 'harbor': 69,\n",
       " 'life': 102,\n",
       " 'nowhere': 124,\n",
       " 'else': 46,\n",
       " 'least': 101,\n",
       " 'near': 121,\n",
       " 'future': 65,\n",
       " 'which': 195,\n",
       " 'migrate': 113,\n",
       " 'visit': 190,\n",
       " 'yes': 200,\n",
       " 'settle': 154,\n",
       " 'not': 123,\n",
       " 'yet': 201,\n",
       " 'like': 104,\n",
       " 'or': 130,\n",
       " 'for': 60,\n",
       " 'moment': 115,\n",
       " 'where': 194,\n",
       " 'make': 110,\n",
       " 'stand': 163,\n",
       " 'has': 70,\n",
       " 'been': 10,\n",
       " 'said': 149,\n",
       " 'astronomy': 7,\n",
       " 'humbling': 83,\n",
       " 'character': 18,\n",
       " 'building': 15,\n",
       " 'experience': 54,\n",
       " 'perhaps': 137,\n",
       " 'better': 12,\n",
       " 'demonstration': 36,\n",
       " 'folly': 59,\n",
       " 'conceits': 23,\n",
       " 'than': 170,\n",
       " 'distant': 38,\n",
       " 'image': 86,\n",
       " 'tiny': 180,\n",
       " 'me': 112,\n",
       " 'underscores': 183,\n",
       " 'responsibility': 147,\n",
       " 'deal': 34,\n",
       " 'more': 118,\n",
       " 'kindly': 96,\n",
       " 'with': 198,\n",
       " 'preserve': 144,\n",
       " 'cherish': 19,\n",
       " 'blue': 14,\n",
       " 've': 188}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4752a879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e55fcf",
   "metadata": {},
   "source": [
    "If we want the CountVectorizer to build a vocabulary (here: features/columns) based only on lemmatized tokens that are not stopwords, we need to specify what kind of preprocessing we want (by deafult, the CountVectorizer method does not apply stemming/lemmatization). Write a function called `my_preprocessor` which given a string, returns another string after tokenization, stopword removal and lemmatization (case for stemming is equivalent) has been applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebf84f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_preprocessor(text):\n",
    "    \n",
    "    tokenized = tokenizer.tokenize(text)\n",
    "    filtered = [word.lower() for word in tokenized if word.lower() not in stop_list]\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in filtered]\n",
    "    return ' '.join(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582e575f",
   "metadata": {},
   "source": [
    "Apply the CountVectorizer to the original corpus (here: `sentences`) with using your own preprocessor `my_preprocessor`. I.e., set the parameter value `preprocessor=my_preprocessor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc799600",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_2 = CountVectorizer(preprocessor=my_preprocessor)\n",
    "corpus_vectors = bow_2.fit(sentences) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac0a0a",
   "metadata": {},
   "source": [
    "Extract the vocabularies/features of the two BoW models you created (one with the deafult parameters and one using your own preprocessor). What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a32078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use method get_feature_names_out() on the countVectorizer model to access the feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63479967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['again' 'aggregate' 'all' 'and' 'another' 'are' 'arena' 'astronomy' 'at'\n",
      " 'become' 'been' 'being' 'better' 'blood' 'blue' 'building' 'by'\n",
      " 'challenged' 'character' 'cherish' 'child' 'civilization' 'come'\n",
      " 'conceits' 'confident' 'corner' 'corrupt' 'cosmic' 'could' 'couple'\n",
      " 'coward' 'creator' 'cruelties' 'dark' 'deal' 'delusion' 'demonstration'\n",
      " 'destroyer' 'distant' 'distinguishable' 'doctrines' 'dot' 'dust' 'eager'\n",
      " 'earth' 'economic' 'else' 'elsewhere' 'emperors' 'endless' 'enveloping'\n",
      " 'ever' 'every' 'everyone' 'experience' 'explorer' 'far' 'father'\n",
      " 'fervent' 'folly' 'for' 'forager' 'fraction' 'frequent' 'from' 'future'\n",
      " 'generals' 'glory' 'great' 'harbor' 'has' 'hatreds' 'have' 'heard' 'help'\n",
      " 'here' 'hero' 'hint' 'history' 'home' 'hopeful' 'how' 'human' 'humbling'\n",
      " 'hunter' 'ideologies' 'image' 'imagined' 'importance' 'in' 'inhabitants'\n",
      " 'inventor' 'is' 'it' 'joy' 'kill' 'kindly' 'king' 'know' 'known' 'leader'\n",
      " 'least' 'life' 'light' 'like' 'lived' 'lives' 'lonely' 'look' 'love'\n",
      " 'make' 'masters' 'me' 'migrate' 'misunderstandings' 'moment' 'momentary'\n",
      " 'morals' 'more' 'mote' 'mother' 'near' 'no' 'not' 'nowhere' 'obscurity'\n",
      " 'of' 'on' 'one' 'only' 'or' 'other' 'our' 'ourselves' 'out' 'pale'\n",
      " 'peasant' 'perhaps' 'pixel' 'planet' 'point' 'politician' 'position'\n",
      " 'posturings' 'preserve' 'privileged' 'religions' 'responsibility'\n",
      " 'rivers' 'said' 'saint' 'save' 'scarcely' 'self' 'settle' 'sinner'\n",
      " 'small' 'so' 'some' 'species' 'speck' 'spilled' 'stage' 'stand'\n",
      " 'suffering' 'sunbeam' 'superstar' 'supreme' 'suspended' 'teacher' 'than'\n",
      " 'that' 'the' 'their' 'there' 'they' 'think' 'this' 'those' 'thousands'\n",
      " 'tiny' 'to' 'triumph' 'underscores' 'universe' 'us' 'vast' 'vastness'\n",
      " 've' 'very' 'visit' 'visited' 'was' 'we' 'where' 'which' 'who' 'will'\n",
      " 'with' 'world' 'yes' 'yet' 'you' 'young']\n",
      "['aggregate' 'another' 'arena' 'astronomy' 'become' 'better' 'blood'\n",
      " 'blue' 'building' 'challenged' 'character' 'cherish' 'child'\n",
      " 'civilization' 'come' 'conceit' 'confident' 'corner' 'corrupt' 'cosmic'\n",
      " 'could' 'couple' 'coward' 'creator' 'cruelty' 'dark' 'deal' 'delusion'\n",
      " 'demonstration' 'destroyer' 'distant' 'distinguishable' 'doctrine' 'dot'\n",
      " 'dust' 'eager' 'earth' 'economic' 'else' 'elsewhere' 'emperor' 'endless'\n",
      " 'enveloping' 'ever' 'every' 'everyone' 'experience' 'explorer' 'far'\n",
      " 'father' 'fervent' 'folly' 'forager' 'fraction' 'frequent' 'future'\n",
      " 'general' 'glory' 'great' 'harbor' 'hatred' 'heard' 'help' 'hero' 'hint'\n",
      " 'history' 'home' 'hopeful' 'human' 'humbling' 'hunter' 'ideology' 'image'\n",
      " 'imagined' 'importance' 'inhabitant' 'inventor' 'joy' 'kill' 'kindly'\n",
      " 'king' 'know' 'known' 'leader' 'least' 'life' 'light' 'like' 'lived'\n",
      " 'lonely' 'look' 'love' 'make' 'master' 'migrate' 'misunderstanding'\n",
      " 'moment' 'momentary' 'moral' 'mote' 'mother' 'near' 'nowhere' 'obscurity'\n",
      " 'one' 'pale' 'peasant' 'perhaps' 'pixel' 'planet' 'point' 'politician'\n",
      " 'position' 'posturing' 'preserve' 'privileged' 'religion'\n",
      " 'responsibility' 'river' 'said' 'saint' 'save' 'scarcely' 'self' 'settle'\n",
      " 'sinner' 'small' 'specie' 'speck' 'spilled' 'stage' 'stand' 'suffering'\n",
      " 'sunbeam' 'superstar' 'supreme' 'suspended' 'teacher' 'think' 'thousand'\n",
      " 'tiny' 'triumph' 'underscore' 'universe' 'vast' 'vastness' 'visit'\n",
      " 'visited' 'world' 'yes' 'yet' 'young']\n"
     ]
    }
   ],
   "source": [
    "print(bow.get_feature_names_out())\n",
    "print(bow_2.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "111a59e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n",
      "152\n"
     ]
    }
   ],
   "source": [
    "print(len(bow.get_feature_names_out()))\n",
    "print(len(bow_2.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ee9b3",
   "metadata": {},
   "source": [
    "The encoding using our own preprocessing steps has a smaller vocabulary / fewer dimensions. This makes sense since our own preprocessing method additionally applies lemmatization, which may map different raw words onto the same lemma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e07210",
   "metadata": {},
   "source": [
    "Use either each the two BoW models to show the encoding of the fifth sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "990da5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:  On it everyone you love, everyone you know, everyone you ever heard of, every human being who ever was, lived out their lives.\n",
      "BoW encoding without lemmatization: [[0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0\n",
      "  0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 3 0]]\n",
      "BoW encoding with lemmatization: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 2 1 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "s = sentences[4]\n",
    "print('Original sentence: ', s)\n",
    "print('BoW encoding without lemmatization:', bow.transform([s]).toarray())\n",
    "print('BoW encoding with lemmatization:', bow_2.transform([s]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d434e1",
   "metadata": {},
   "source": [
    "### Feature extraction method: Set of Words (using CountVectorizor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a07ce7",
   "metadata": {},
   "source": [
    "We want to use the CountVectorizer method to encode our text. This time, for each word in the vocabulary, we are only interested in whether the word appears or not in a given document. The frequency is not important. This encoding is also called Set of Words. Find the parameter in the CountVectorizer method that produces such a 0/1 encoding. Set the preprocessor parameter to the preprocessor defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46d36c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sow = CountVectorizer(binary=True, preprocessor=my_preprocessor)\n",
    "corpus_vectors = sow.fit(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d035d2",
   "metadata": {},
   "source": [
    "Show the encoding of the fifth sentence in the text using the Set of Words model. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f82cd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:  On it everyone you love, everyone you know, everyone you ever heard of, every human being who ever was, lived out their lives.\n",
      "SoW encoding: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "s = sentences[4]\n",
    "print('Original sentence: ', s)\n",
    "print('SoW encoding:', sow.transform([s]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d3ff23",
   "metadata": {},
   "source": [
    "All numbers are 0/1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd686e30",
   "metadata": {},
   "source": [
    "### Feature extraction method: Tf-idf scores (using TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db4875c",
   "metadata": {},
   "source": [
    "We want to use the TfidfVectorizer method to encode our text. This time, for each word in the vocabulary, we are interested in the tf-idf score of the word in a given document. Set the preprocessor parameter to the preprocessor defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d67ae79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c239837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(preprocessor=my_preprocessor)\n",
    "corpus_vectors = tfidf.fit_transform(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a4833d",
   "metadata": {},
   "source": [
    "Show the encoding of the fifth sentence is the text using the TfIdf model. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b97ca463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:  On it everyone you love, everyone you know, everyone you ever heard of, every human being who ever was, lived out their lives.\n",
      "SoW encoding: [[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.41490215 0.20745108 0.70801182 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.23600394 0.         0.         0.         0.\n",
      "  0.         0.         0.20745108 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.23600394 0.         0.\n",
      "  0.         0.20745108 0.         0.         0.20745108 0.\n",
      "  0.         0.20745108 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "s = sentences[4]\n",
    "print('Original sentence: ', s)\n",
    "print('SoW encoding:', tfidf.transform([s]).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d33e13",
   "metadata": {},
   "source": [
    "All numbers are between 0 and 1, as expected for the range of the tf-idf scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf0651a",
   "metadata": {},
   "source": [
    "### Feature extraction method: Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0e1061",
   "metadata": {},
   "source": [
    "Word2vec is one of the most popular techniques to learn word embeddings using a two-layer neural network. The input layer contains the individual words which we want to compress, the output layer can be its surrounding context. The embedding maps the words into a vector space which has a dimension smaller than the size of the vocabulary. A well-trained set of word vectors will place similar words close to each other in that (vector) space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbfb1cb",
   "metadata": {},
   "source": [
    "The input of the Gensim word2vec is a text corpus and its output is a set of vectors. More specifically, genism word2vec requires a format of â€˜list of listsâ€™ for training where every document is contained in a list and every inner list contains lists of tokens of that document. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28acb4f1",
   "metadata": {},
   "source": [
    "Create a word2vec embedding for the lemmatized corpus such that the compressed vector has a dimension of 20 and every word that appears at least once in the corpus must be considered. Use the skip-gram algorithm and consider all surrounding words of distance less or equal 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6729d378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\bakullari\\anaconda3\\envs\\env-ids2022-23\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\bakullari\\anaconda3\\envs\\env-ids2022-23\\lib\\site-packages (from gensim) (6.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\bakullari\\anaconda3\\envs\\env-ids2022-23\\lib\\site-packages (from gensim) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\bakullari\\anaconda3\\envs\\env-ids2022-23\\lib\\site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\bakullari\\anaconda3\\envs\\env-ids2022-23\\lib\\site-packages (from gensim) (0.29.28)\n"
     ]
    }
   ],
   "source": [
    "# run this cell to install the gensim package\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5de553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b24c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_size: The number of dimensions of the embeddings and the default is 100.\n",
    "# window: The maximum distance between a target word and words around the target word. The default window is 5.\n",
    "# min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.\n",
    "# sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.\n",
    "\n",
    "w2v = Word2Vec(corpus_lemma, min_count=1, vector_size=20, window=3, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1260240",
   "metadata": {},
   "source": [
    "Obtain the embeddings of the words: future, blue, universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f441b476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "future: [-0.01134635 -0.02719816  0.03814759  0.03324739 -0.02231137  0.01154214\n",
      " -0.02944485  0.00161118  0.04687099 -0.0130652  -0.02552955 -0.03749329\n",
      " -0.01477422 -0.00420845  0.01746522  0.0486251  -0.01622649  0.00980703\n",
      "  0.04809051  0.00712067]\n",
      "blue: [-0.02614041 -0.03331963 -0.03823137  0.04199896 -0.01039955 -0.03447679\n",
      " -0.02071675  0.02617425 -0.01466679 -0.01884807  0.00858427 -0.01437992\n",
      " -0.00827299  0.00570001 -0.01502686  0.04251807  0.01988973 -0.0494634\n",
      "  0.03117746 -0.03388793]\n",
      "universe: [-0.02200732 -0.04642254 -0.0087069  -0.01776312  0.04506843  0.01419324\n",
      " -0.02991859 -0.01620416 -0.0500894   0.00952429 -0.01948267 -0.01419335\n",
      "  0.02536843  0.03787052  0.02165071 -0.03425599  0.03424682 -0.04766918\n",
      " -0.03521582 -0.04028212]\n"
     ]
    }
   ],
   "source": [
    "word_vectors = w2v.wv\n",
    "print('future:', word_vectors['future'])\n",
    "print('blue:', word_vectors['blue'])\n",
    "print('universe:', word_vectors['universe'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3104d2c2",
   "metadata": {},
   "source": [
    "Which words are the most similar ones to the words future, blue and universe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9dd00fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for future: astronomy\n",
      "for blue: pixel\n",
      "for universe: corner\n"
     ]
    }
   ],
   "source": [
    "future_similar = word_vectors.similar_by_word(\"future\")\n",
    "most_similar_key, similarity = future_similar[0]  # look at the first match\n",
    "print(\"for future:\", most_similar_key)\n",
    "\n",
    "blue_similar = word_vectors.similar_by_word(\"blue\")\n",
    "most_similar_key, similarity = blue_similar[0]  # look at the first match\n",
    "print(\"for blue:\", most_similar_key)\n",
    "\n",
    "universe_similar = word_vectors.similar_by_word(\"universe\")\n",
    "most_similar_key, similarity = universe_similar[0]  # look at the first match\n",
    "print(\"for universe:\", most_similar_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f51135",
   "metadata": {},
   "source": [
    "### Feature extraction method: Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b03d1a5",
   "metadata": {},
   "source": [
    "Import the Fake.csv and the True.csv files into two separate dataframes. Those files contain news that were published that were either fake or real. Display the first few lines of those dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a56c5978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Yearâ€™...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obamaâ€™s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Yearâ€™...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obamaâ€™s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fake_news = pd.read_csv(\"Fake.csv\")\n",
    "real_news = pd.read_csv(\"True.csv\")\n",
    "\n",
    "fake_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be48ee8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date  \n",
       "0  December 31, 2017   \n",
       "1  December 29, 2017   \n",
       "2  December 31, 2017   \n",
       "3  December 30, 2017   \n",
       "4  December 29, 2017   "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5269f1",
   "metadata": {},
   "source": [
    "For each of those dataframes, remove the \"subject\" and the \"date\" columns. For each of the two dataframes, add a new column named \"label\", and set its value to \"real\" for the real news dataframe, and to \"fake\" for the fake news dataframe. Display the first few lines of the modified dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c2bd55ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news.drop(['subject', 'date'], axis=1, inplace=True)\n",
    "fake_news['label'] = 'fake'\n",
    "real_news.drop(['subject', 'date'], axis=1, inplace=True)\n",
    "real_news['label'] = 'real'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "459dfca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Yearâ€™...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obamaâ€™s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Yearâ€™...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obamaâ€™s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text label  \n",
       "0  Donald Trump just couldn t wish all Americans ...  fake  \n",
       "1  House Intelligence Committee Chairman Devin Nu...  fake  \n",
       "2  On Friday, it was revealed that former Milwauk...  fake  \n",
       "3  On Christmas day, Donald Trump announced that ...  fake  \n",
       "4  Pope Francis used his annual Christmas Day mes...  fake  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8fc4cc8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text label  \n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  real  \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  real  \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  real  \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  real  \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  real  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033679f7",
   "metadata": {},
   "source": [
    "Concatenate (merge) the two dataframes into a single dataframe named `news_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b6e1742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Yearâ€™...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obamaâ€™s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21412</th>\n",
       "      <td>'Fully committed' NATO backs new U.S. approach...</td>\n",
       "      <td>BRUSSELS (Reuters) - NATO allies on Tuesday we...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21413</th>\n",
       "      <td>LexisNexis withdrew two products from Chinese ...</td>\n",
       "      <td>LONDON (Reuters) - LexisNexis, a provider of l...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21414</th>\n",
       "      <td>Minsk cultural hub becomes haven from authorities</td>\n",
       "      <td>MINSK (Reuters) - In the shadow of disused Sov...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21415</th>\n",
       "      <td>Vatican upbeat on possibility of Pope Francis ...</td>\n",
       "      <td>MOSCOW (Reuters) - Vatican Secretary of State ...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21416</th>\n",
       "      <td>Indonesia to buy $1.14 billion worth of Russia...</td>\n",
       "      <td>JAKARTA (Reuters) - Indonesia will buy 11 Sukh...</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0       Donald Trump Sends Out Embarrassing New Yearâ€™...   \n",
       "1       Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2       Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3       Trump Is So Obsessed He Even Has Obamaâ€™s Name...   \n",
       "4       Pope Francis Just Called Out Donald Trump Dur...   \n",
       "...                                                  ...   \n",
       "21412  'Fully committed' NATO backs new U.S. approach...   \n",
       "21413  LexisNexis withdrew two products from Chinese ...   \n",
       "21414  Minsk cultural hub becomes haven from authorities   \n",
       "21415  Vatican upbeat on possibility of Pope Francis ...   \n",
       "21416  Indonesia to buy $1.14 billion worth of Russia...   \n",
       "\n",
       "                                                    text label  \n",
       "0      Donald Trump just couldn t wish all Americans ...  fake  \n",
       "1      House Intelligence Committee Chairman Devin Nu...  fake  \n",
       "2      On Friday, it was revealed that former Milwauk...  fake  \n",
       "3      On Christmas day, Donald Trump announced that ...  fake  \n",
       "4      Pope Francis used his annual Christmas Day mes...  fake  \n",
       "...                                                  ...   ...  \n",
       "21412  BRUSSELS (Reuters) - NATO allies on Tuesday we...  real  \n",
       "21413  LONDON (Reuters) - LexisNexis, a provider of l...  real  \n",
       "21414  MINSK (Reuters) - In the shadow of disused Sov...  real  \n",
       "21415  MOSCOW (Reuters) - Vatican Secretary of State ...  real  \n",
       "21416  JAKARTA (Reuters) - Indonesia will buy 11 Sukh...  real  \n",
       "\n",
       "[44898 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [fake_news, real_news]\n",
    "news_df = pd.concat(frames)\n",
    "\n",
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b540f5f",
   "metadata": {},
   "source": [
    "Create a corpus named `corpus` containing a list of all documents. Each document is a text field under \"text\" from the dataframe `news_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "80c9ffb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Yearâ€™...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obamaâ€™s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Yearâ€™...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obamaâ€™s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text label  \n",
       "0  Donald Trump just couldn t wish all Americans ...  fake  \n",
       "1  House Intelligence Committee Chairman Devin Nu...  fake  \n",
       "2  On Friday, it was revealed that former Milwauk...  fake  \n",
       "3  On Christmas day, Donald Trump announced that ...  fake  \n",
       "4  Pope Francis used his annual Christmas Day mes...  fake  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be6e793d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus =  news_df['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c569e1",
   "metadata": {},
   "source": [
    "Create a preprocessed version of the corpus and save it under `corpus_p`. This corpus should be a list of documents. Each document is a list of terms. You must get to this list of terms after applying your previously defined preprocessor function to the document. Display some document contained in the preprocessed corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "781399b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_p = []\n",
    "for doc in corpus:\n",
    "    doc_p = my_preprocessor(doc)\n",
    "    corpus_p.append(doc_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4932ba94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'donald trump wish american happy new year leave instead give shout enemy hater dishonest fake news medium former reality show star one job country rapidly grows stronger smarter want wish friend supporter enemy hater even dishonest fake news medium happy healthy new year president angry pant tweeted 2018 great year america country rapidly grows stronger smarter want wish friend supporter enemy hater even dishonest fake news medium happy healthy new year 2018 great year america donald j trump realdonaldtrump december 31 2017trump tweet went welll expect kind president sends new year greeting like despicable petty infantile gibberish trump lack decency even allow rise gutter long enough wish american citizen happy new year bishop talbert swan talbertswan december 31 2017no one like calvin calvinstowell december 31 2017your impeachment would make 2018 great year america also accept regaining control congress miranda yaver mirandayaver december 31 2017do hear talk include many people hate wonder hate alan sandoval alansandoval13 december 31 2017who us word hater new year wish marlene marlene399 december 31 2017you say happy new year koren pollitt korencarpenter december 31 2017here trump new year eve tweet 2016 happy new year including many enemy fought lost badly know love donald j trump realdonaldtrump december 31 2016this nothing new trump year trump directed message enemy hater new year easter thanksgiving anniversary 9 11 pic twitter com 4fpae2kypa daniel dale ddale8 december 31 2017trump holiday tweet clearly presidential long work hallmark becoming president steven goodine sgoodine december 31 2017he always like difference last year filter breaking roy schulze thbthttt december 31 2017who apart teenager us term hater wendy wendywhistles december 31 2017he fucking 5 year old know rainyday80 december 31 2017so people voted hole thinking would change got power wrong 70 year old men change year older photo andrew burton getty image'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_p[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920823a0",
   "metadata": {},
   "source": [
    "Create a doc2vec embedding of the documents in news_df with a vector size of 50. Consider all words that appear at least 10 times in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cdac484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3ad9bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TaggedDocument\n",
    "# note how the tag of each document is some unique identifier for the document, \n",
    "# for simplicity, we set it to the doc's position in the corpus\n",
    "docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus_p)]\n",
    "\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# determining parameters of the model   \n",
    "doc2vec = Doc2Vec(vector_size=50, min_count=10, workers=cores)\n",
    "# building the vocabulary    \n",
    "doc2vec.build_vocab(docs)\n",
    "\n",
    "\n",
    "# document embedding, create the embedding based on all documents in the corpus\n",
    "doc2vec.train(corpus_iterable=docs, total_examples=doc2vec.corpus_count, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d2d63",
   "metadata": {},
   "source": [
    "Display the vector corresponding to the first document. Find its most similar document w.r.t. the cosine similarity and display the original text corresponding to those two docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb32f324",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1= corpus_p[0]\n",
    "# infer_vector requires the sentence (=document) to be passed as a list of tokens\n",
    "d1_tokens = tokenizer.tokenize(d1)\n",
    "# print(d1_tokens)\n",
    "d1_embedding = doc2vec.infer_vector(d1_tokens)\n",
    "most_similar_docs = doc2vec.dv.most_similar(d1_embedding) #gives you top 10 document tags and their cosine similarity\n",
    "tag = most_similar_docs[0][0] #obtain the tag of the first document'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ef01d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_original = news_df.iloc[0]['text']\n",
    "d_similar_original = news_df.iloc[tag]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2406d736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and  the very dishonest fake news media.  The former reality show star had just one job to do and he couldn t do it. As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year,  President Angry Pants tweeted.  2018 will be a great year for America! As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year. 2018 will be a great year for America!  Donald J. Trump (@realDonaldTrump) December 31, 2017Trump s tweet went down about as welll as you d expect.What kind of president sends a New Year s greeting like this despicable, petty, infantile gibberish? Only Trump! His lack of decency won t even allow him to rise above the gutter long enough to wish the American citizens a happy new year!  Bishop Talbert Swan (@TalbertSwan) December 31, 2017no one likes you  Calvin (@calvinstowell) December 31, 2017Your impeachment would make 2018 a great year for America, but I ll also accept regaining control of Congress.  Miranda Yaver (@mirandayaver) December 31, 2017Do you hear yourself talk? When you have to include that many people that hate you you have to wonder? Why do the they all hate me?  Alan Sandoval (@AlanSandoval13) December 31, 2017Who uses the word Haters in a New Years wish??  Marlene (@marlene399) December 31, 2017You can t just say happy new year?  Koren pollitt (@Korencarpenter) December 31, 2017Here s Trump s New Year s Eve tweet from 2016.Happy New Year to all, including to my many enemies and those who have fought me and lost so badly they just don t know what to do. Love!  Donald J. Trump (@realDonaldTrump) December 31, 2016This is nothing new for Trump. He s been doing this for years.Trump has directed messages to his  enemies  and  haters  for New Year s, Easter, Thanksgiving, and the anniversary of 9/11. pic.twitter.com/4FPAe2KypA  Daniel Dale (@ddale8) December 31, 2017Trump s holiday tweets are clearly not presidential.How long did he work at Hallmark before becoming President?  Steven Goodine (@SGoodine) December 31, 2017He s always been like this . . . the only difference is that in the last few years, his filter has been breaking down.  Roy Schulze (@thbthttt) December 31, 2017Who, apart from a teenager uses the term haters?  Wendy (@WendyWhistles) December 31, 2017he s a fucking 5 year old  Who Knows (@rainyday80) December 31, 2017So, to all the people who voted for this a hole thinking he would change once he got into power, you were wrong! 70-year-old men don t change and now he s a year older.Photo by Andrew Burton/Getty Images.\n",
      "\n",
      "\n",
      "WASHINGTON (Reuters) - President Barack Obama selected Merrick Garland for the U.S. Supreme Court on Wednesday, choosing a centrist judge meant to win over recalcitrant Senate Republicans whose leaders wasted no time in spurning the Democratic president. A bruising political fight is brewing over the nomination, which also promises to figure in the already contentious campaign for the Nov. 8 U.S. presidential election. The Republican-led Senateâ€™s leaders have vowed not to hold confirmation hearings or an up-or-down vote on any Obama nominee. Garland, 63, was picked to replace long-serving conservative Justice Antonin Scalia, who died on Feb. 13. A Chicagoan like Obama, he serves as chief judge of the influential U.S. Court of Appeals for the District of Columbia Circuit and is a former prosecutor who in the past has won praise from both Republicans and Democrats. Wasting no time in pressing its case for Senate confirmation, the administration is dispatching Garland to Capitol Hill on Thursday to huddle with Senator Patrick Leahy of Vermont, the senior Judiciary Committee Democrat and then with Senate Democratic leader Harry Reid of Nevada. Such meetings are aimed at shoring up Senate support for the nominee and generating media coverage. The lifetime appointment to the high court requires Senate confirmation. Obamaâ€™s announcement prompted a flood of reaction from private groups that will work to advance or kill the nomination. The UAW, representing automobile, aerospace and some agricultural workers, call Garland â€œa distinguished, moderate judge with more federal judicial experience than any other Supreme Court nominee in history.â€ National Rifle Association Executive Director Chris Cox said, â€œA basic analysis of Merrick Garlandâ€™s judicial record shows that he does not respect our fundamental, individual right to keep and bear arms for self-defense.â€   Republicans, hoping a candidate from their party wins the presidential election, are demanding that Obama leave the seat vacant and let his successor, to be sworn in next January, make the selection. Billionaire businessman Donald Trump is leading among Republicans for the nomination. Obamaâ€™s former secretary of state, Hillary Clinton, is the front-runner for the Democrats. Obama said Republican senators should give Garland a fair hearing. He said that failing to do so â€œwill not only be an abdication of the Senateâ€™s constitutional duty, it will indicate a process for nominating and confirming judges that is beyond repair.â€ Such a move, he said, would also undermine the reputation of the Supreme Court and faith in the American justice system. â€œOur democracy will ultimately suffer as well,â€ Obama added, as he introduced Garland at a White House Rose Garden ceremony. Scaliaâ€™s death left the nine-member Supreme Court evenly split with four liberals and four conservative justices. Obamaâ€™s nominee could tilt the court to the left for the first time in decades, which could affect rulings on contentious issues including abortion, gun rights, the death penalty and political spending. Obama said the Supreme Court was supposed to be above politics and it should remain so. Obama said that with politics in the United States so polarized, â€œthis is precisely the time when we should play it straight, and treat the process of appointing a Supreme Court justice with the seriousness and care it deserves.â€ Senate Majority Leader Mitch McConnell of Kentucky swiftly reiterated that the Senate will not consider the nomination by the president. A McConnell spokesman said the senator had spoken by phone with Garland and would not hold a â€œperfunctory meetingâ€ with him.  John Cornyn of Texas, the second-ranking Senate Republican, added, â€œThis person will not be confirmed, so thereâ€™s no reason going through some motions and pretending like it will happen, because itâ€™s not going to happen.â€ Some cracks began appearing in McConnellâ€™s strategy of completely shutting out the nominee. A handful of Republican senators including Susan Collins of Maine, Kelly Ayotte of New Hampshire, Jeff Flake of Arizona, Mark Kirk of Illinois and Rob Portman of Ohio said they would be willing to meet with Garland. Collins said the Senate Judiciary Committee should hold confirmation hearings. Judiciary Committee member Orrin Hatch, whose past support of Garland was cited by Obama, said the pick does not change his view â€œat this pointâ€ that no Obama nominee should be considered.  Senator Pat Toomey of Pennsylvania, who is in a tough re-election battle, said, â€œShould Merrick Garland be nominated again by the next president, I would be happy to carefully consider his nomination.â€ Garland is the oldest Supreme Court nominee since Republican Richard Nixon in 1971 nominated Lewis Powell, who was 64. Presidents tend to pick nominees younger than that so they can serve for decades and extend a presidentâ€™s legacy. Obama may reason that the choice of an older nominee might also entice Senate Republicans into considering his selection. Garland would become the fourth Jewish member of the nine-member court. There are five Roman Catholics on the court. Obama considered but passed over Garland when he made two prior Supreme Court appointments.  With solid Republican support, the Senate voted in 1997 to confirm Garland to his present job in a bipartisan 76-23 vote after he was nominated by Democratic President Bill Clinton. Garland is widely viewed as a moderate. He is a former prosecutor who served in the Justice Department under Clinton. He oversaw the prosecution in the 1995 Oklahoma City bombing case including securing the death penalty for the lead defendant, anti-government militant Timothy McVeigh. In his current post, he is known for narrow, centrist opinions and rhetoric that is measured rather than inflammatory even when in dissent. Standing in between Obama and Vice President Joe Biden during the Rose Garden ceremony, an emotional Garland referred to the Oklahoma City bombing case, saying, â€œOnce again, I saw the importance of assuring victims and families that the justice system could work.â€ Obama said he fulfilled his constitutional duty by naming a nominee and said it was time for the Senate to do its job.  â€œPresidents do not stop working in the final year of their term. Neither should a senator,â€ Obama said. Obama, in office since 2009, has already named two justices to the Supreme Court: Sonia Sotomayor, who at 55 became the first Hispanic justice in 2009, and Elena Kagan, who was 50 when she became the fourth woman ever to serve on the court in 2010. Democrats praised his latest choice. â€œIf Merrick Garland canâ€™t get bipartisan support no one can,â€ Democratic Senator Charles Schumer of New York said. Hillary Clinton called Garland â€œa brilliant legal mind,â€ urging the Senate to move ahead with the confirmation process. Trump said it was critical for Republicans to take back the White House to avoid Democrats shaping the Supreme Court for decades to come. \n"
     ]
    }
   ],
   "source": [
    "print(d1_original)\n",
    "print(\"\\n\")\n",
    "print(d_similar_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b537cb33",
   "metadata": {},
   "source": [
    "## Classification Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9503e7c3",
   "metadata": {},
   "source": [
    "Split the corpus from the previous task into training (80%) and test (20%) data preserving the distribution based on the \"label\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c4aa24b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f5071849",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = news_df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "371ad757",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train, corpus_test, y_train, y_test = train_test_split(corpus, target, test_size=0.2, stratify=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29be598a",
   "metadata": {},
   "source": [
    "### Based on Doc2Vec embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a448add9",
   "metadata": {},
   "source": [
    "For each of the corpora, create their preprocessed versions using your preprocessor defined above. Save them under `corpus_train_p` and `corpus_test_p`. Create a doc2vec model based only on the documents in `corpus_train_p`. Set the vector dimension to 50 and min_count to 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "90b0452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_p = []\n",
    "for doc in corpus_train:\n",
    "    doc_p = my_preprocessor(doc)\n",
    "    corpus_train_p.append(doc_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ee658b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test_p = []\n",
    "for doc in corpus_test:\n",
    "    doc_p = my_preprocessor(doc)\n",
    "    corpus_test_p.append(doc_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "40ea953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TaggedDocument\n",
    "docs_train = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus_train_p)]\n",
    "\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "# determining parameters of the model   \n",
    "doc2vec_from_train = Doc2Vec(vector_size=50, min_count=10, workers=cores)\n",
    "# building the vocabulary    \n",
    "doc2vec_from_train.build_vocab(docs_train)\n",
    "\n",
    "\n",
    "# document embedding, train the model on the training examples\n",
    "doc2vec_from_train.train(corpus_iterable=docs_train, total_examples=doc2vec_from_train.corpus_count, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f12bdb",
   "metadata": {},
   "source": [
    "Use the embedding of the training corpus to train a logistic regression classifier with the label as target feature. Use the classifier to predict the label of the documents in the test corpus and show its accuracy both for the training corpus and the test corpus. \n",
    "<i>Hint: You can assess the encoding of each document using the \"infer_vector\" method. <b> Don't forget to train and test your classifier on the embeddings of the preprocessed documents (and not on the corpus itself). </b> </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f1da4840",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [doc2vec_from_train.infer_vector(tokenizer.tokenize(doc)) for doc in corpus_train_p]\n",
    "X_test = [doc2vec_from_train.infer_vector(tokenizer.tokenize(doc)) for doc in corpus_test_p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "410be8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy on training set: 0.667993763572582\n",
      "Classifier accuracy on test set: 0.6760579064587974\n"
     ]
    }
   ],
   "source": [
    "#classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf_doc2vec = LogisticRegression(max_iter=1000)\n",
    "clf_doc2vec.fit(X_train, y_train)\n",
    "y_train_pred = clf_doc2vec.predict(X_train)\n",
    "y_test_pred = clf_doc2vec.predict(X_test)\n",
    "\n",
    "accuracy_doc2vec_train = accuracy_score(y_train, y_train_pred, normalize=True)\n",
    "print('Classifier accuracy on training set:', accuracy_doc2vec_train)\n",
    "\n",
    "accuracy_doc2vec_test = accuracy_score(y_test, y_test_pred, normalize=True)\n",
    "print('Classifier accuracy on test set:', accuracy_doc2vec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ce1e9a",
   "metadata": {},
   "source": [
    "### Based on BoW encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfa0d8c",
   "metadata": {},
   "source": [
    "Create a Bag of Words model based only on the documents in the training data. Use the previously defined preprocessor as preprocessor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eb84b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_from_train = CountVectorizer(preprocessor=my_preprocessor)\n",
    "bow_from_train.fit(corpus_train) \n",
    "\n",
    "X_train = bow_from_train.transform(corpus_train)\n",
    "X_test = bow_from_train.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b0d272",
   "metadata": {},
   "source": [
    "Use the BoW encoding of the training corpus to train a logistic regression classifier with the label as target feature. Use the classifier to predict the label of the documents in the test corpus and show its accuracy both for the training corpus and the test corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "69ff702f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy on training set: 0.9999721588061696\n",
      "Classifier accuracy on test set: 0.9967706013363029\n"
     ]
    }
   ],
   "source": [
    "clf_bow = LogisticRegression(max_iter=1000)\n",
    "clf_bow.fit(X_train, y_train)\n",
    "y_train_pred = clf_bow.predict(X_train)\n",
    "y_test_pred = clf_bow.predict(X_test)\n",
    "\n",
    "accuracy_bow_train = accuracy_score(y_train, y_train_pred, normalize=True)\n",
    "print('Classifier accuracy on training set:', accuracy_bow_train)\n",
    "\n",
    "accuracy_bow_test = accuracy_score(y_test, y_test_pred, normalize=True)\n",
    "print('Classifier accuracy on test set:', accuracy_bow_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb623e5",
   "metadata": {},
   "source": [
    "<b>Important:</b>\n",
    "- We split the data into training and test data once and used that same partition for both the Doc2Vec based classifier and BoW based classifier. This is important if we want to make claims comparing how the classifier performs on one feature extraction method compared to the other.\n",
    "- The feature extraction was computed based only on the training corpus. This is important if we want to really test the classifier on independent data and reason about how \"well\" the embedding generalizes even for unseen text. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
