{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction: Data Preprocessing and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In previous weeks, we have already seen several data preprocessing methods. Recall, for example, the instruction  on data exploration, basic visualization, and decison trees or the instruction session on regression. We learned preprocessing steps such as\n",
    "\n",
    "* One-hot encoding\n",
    "* Outlier detection\n",
    "* Handling missing values\n",
    "* etc.\n",
    "\n",
    "Today we will see a few more techniques. However, note that there are many possiblities to achieve your preprocessing goals in Python and also this instruction just shows a subset of those.\n",
    "\n",
    "## Missing values\n",
    "\n",
    "To illustrate the handling of missing values we will use a running example based on the *diabetes data set*. The data set has the following attributes:\n",
    "\n",
    "| Column | Description |\n",
    "| ------ | ----------- |\n",
    "|Pregnant | Number of times pregnant |\n",
    "|PGC | Plasma glucose concentration a 2 hours in an oral glucose tolerance test |\n",
    "|BP | Diastolic blood pressure (mm Hg) |\n",
    "|Triceps | Triceps skinfold thickness (mm) |\n",
    "|Insulin | 2-Hour serum insulin (mu U/ml) |\n",
    "|BMI | Body mass index (weight in kg/(height in m)^2) |\n",
    "|Pedi | Diabetes pedigree function |\n",
    "|Age | Age (years) |\n",
    "|Diabetes | Diabetes (yes/no) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import the data set, have a look at the first couple rows, and check if `pandas` already infered reasonable data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('pima-indians-diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)\n",
    "display(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.describe())\n",
    "print(f'Total number of NAN values: {df.isna().sum().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the datatypes seem to be reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the `describe()` method to print summary statistics on the data set to get a rough overview over the data.\n",
    "Moreover, we check for existing NaN entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This summary can already help us to identify missing values (or the presence of potentially incorrectly recorded data, e.g., unrealistic min/max values). Recall from the lecture, not all missing values will be denoted by NaN. In fact, there are no NaN entries in this dataset. From the summary, we can see that several attributes have a minimum value of 0. From this, combined with domain knowledge, we can conclude that for several attributes the '0' value represents a missing/incorrect value.\n",
    "These attributes are:\n",
    "1. Plasma glucose concentration (PGC)\n",
    "2. Diastolic blood pressure (BP)\n",
    "3. Triceps skinfold thickness (Triceps)\n",
    "4. 2-Hour serum insulin (Insulin)\n",
    "5. Body mass index (BMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can \"blindly\" fill these values, we investigate the missing values further.\n",
    "To this end, we empirically investigate a few rows that are affected by missing values and count the number of missing values per dimension of the dataframe.\n",
    "The latter helps us to identify attributes and rows that have suspiciously many NaN entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df[['PGC', 'BP', 'Triceps', 'Insulin', 'BMI']] == 0\n",
    "display(df.loc[df_tmp.any(axis=1)])\n",
    "print('Colums NaNs')\n",
    "display(df_tmp.sum(axis=0))\n",
    "print('Row NaNs')\n",
    "display(df_tmp.sum(axis=1).value_counts())\n",
    "print(f'Total number of rows: {len(df_tmp.index)}')\n",
    "\n",
    "del df_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This very basic statistics already yield some interesting insights.\n",
    "First, there are no obviously suspicious NaN patterns within the individual data records.\n",
    "For example, sometimes you may observe that given that a record has missing values, the probability that it has multiple missing values is quite high (i.e., colloquial speaking, if there is a problem somewhere, then there are probably more problems as well).\n",
    "\n",
    "Second, some attributes seem to have severe problems having more than 30% missing values. This, generally, raises the question if it makes sense to consider this attribute at all. (And you should definitely consider this when interpreting your results)\n",
    "However, in this task, for the sake of exercise, we decide to impute the missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we impute the missing values, we transform our data such that the invalid 0 values are actually represented as NaN. This is convenient because NaN values are ignored by functions such as `sum` or `count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace 0 values by NaN values\n",
    "df[['PGC', 'BP', 'Triceps', 'Insulin', 'BMI']] = df[['PGC', 'BP', 'Triceps', 'Insulin', 'BMI']].replace(0, np.NaN)\n",
    "#count the number of NaN values to see whether we replaced all required values\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove data entries with missing values\n",
    "The simplest strategie to handle missing values is simply deleting all records that contain missing values. Pandas provides a simple function to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of rows and columns of the original dataset\n",
    "print(df.shape)\n",
    "# drop rows with missing values\n",
    "#df.dropna(inplace=True)\n",
    "df_tmp = df.dropna()\n",
    "# count the number of rows and columns left in the dataset\n",
    "print(df_tmp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing values\n",
    "Simply removing all data entries that contain missing values reduces the data set significantly. This way, we might loose valuable information important when training models on the data set (e.g. regression models, decision trees etc.). Moreover, it makes our analysis prone to having a survivors bias. Therefore, using other methods might be beneficial: \n",
    "\n",
    "- Filling in a constant value obtained through domain knowledge\n",
    "- A value from another randomly selected record\n",
    "- Mean / median / mode value of the attribute\n",
    "- A value estimated by another predictive model\n",
    "\n",
    "We can use the function `fillna()` from the `pandas` package for simply filling our missing values. The first argument of the function specifies the value that should replace the missing values. In the example below, we use the mean of each column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values with mean column values\n",
    "df_tmp = df.fillna(df.mean(axis=0))\n",
    "# count the number of NaN values in each column\n",
    "print(df_tmp.isna().sum())\n",
    "display(df_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can also choose among different *Imputers* provided by the `sklearn.impute` package. The example below demonstrates its usage. \n",
    "\n",
    "**Note that there are many other imputation methods already implemented in sklearn.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.NaN, strategy = \"mean\")\n",
    "transformed_values = imputer.fit_transform(df)\n",
    "\n",
    "print(np.isnan(transformed_values).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A minor problem when leaving the code above as it is, is that our input was a `pandas.DataFrame` while the output is a `numpy.ndarray`. Depending on our next steps, it might be better to transform this into a `pandas.DataFrame` that is consistent in terms of the selected index (in our current case this is simply 0-n) and the columns. Let's create such a `pandas.DataFrame`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = pd.DataFrame(transformed_values, columns=df.columns, index=df.index)\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Your turn!\n",
    "Handling missing values should have an effect on the quality of our trained models. To show this, take the *class-grades dataset* and train two different linear regression models to predict the final grade. Compare their accuracy scores.\n",
    "\n",
    "* Model 1 should be trained on a data set with the missing values deleted.\n",
    "* Model 2 should be trained on a data set with the missing values replaced by the mean of the attribute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# solution\n",
    "grades = pd.read_csv('class-grades.csv', sep=\",\")\n",
    "print(grades.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling / Normalization / Standardization\n",
    "\n",
    "The terms scaling, normalization, and standardization are often used interchangeably. Most often normalization aims to rescale values into a certain range (e.g. [0,1]) and standardization rescales data to have a mean of 0 and a standard deviation of 1. **Note, that the `sklearn.preprocessing` package has chosen the general term \"scaling\" for both, normalization and standardization operations.**\n",
    "\n",
    "In general, aside from the name of a function, it is important to know how it transforms your data and whether that is what you are aiming to achieve.\n",
    "\n",
    "We take a closer look at the theory behind some of the scaling approaches in the Pen & Paper part of this instruction. Here, we focus on how to apply scaling to data in Python.\n",
    "\n",
    "\n",
    "### Scaling attributes to a range\n",
    "The *MinMaxScaler* transforms each feature to a given range (default: [0,1])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "data = np.array([[ 1., -1.,  2.], [ 2.,  0.,  0.],[ 0.,  1., -1.]])\n",
    "\n",
    "#creating the scaler\n",
    "min_max_scaler = preprocessing.MinMaxScaler() \n",
    "#fitting the scaler to the data\n",
    "min_max_scaler.fit(data)\n",
    "#printing some information on the fittet scaler\n",
    "print('The minimum value of each feature: ')\n",
    "print(min_max_scaler.data_min_)\n",
    "print('The maximum value of each feature: ')\n",
    "print(min_max_scaler.data_max_)\n",
    "print('The current range of each feature: ')\n",
    "print(min_max_scaler.data_range_)\n",
    "print(' ')\n",
    "\n",
    "\n",
    "#transforming the data to the defined new range\n",
    "transformed_data = min_max_scaler.transform(data)\n",
    "print('The rescaled data: ')\n",
    "print(transformed_data)\n",
    "\n",
    "#We can also transform data other than the ones used to fit the scaler\n",
    "print(min_max_scaler.transform([[2,2,2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the scaling function is bijective, we can also easily revert the scaling by using the `inverse_transform` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale\n",
    "data_rescaled = min_max_scaler.inverse_transform(transformed_data)\n",
    "# Check if the rescaled array is close (by a certain tolerance) to the original data\n",
    "assert np.allclose(data, data_rescaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *MaxAbsScaler* scales each attribute such that the maximum absolute value of each feature in the training set will be 1.0. It's functions are similar to the MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[ 1., -1.,  2.], [ 2.,  0.,  0.],[ 0.,  1., -1.]])\n",
    "\n",
    "#creating the scaler\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler() \n",
    "max_abs_scaler.fit(data)\n",
    "\n",
    "#transforming the data to the defined new range\n",
    "transformed_data = max_abs_scaler.transform(data)\n",
    "print('The rescaled data: ')\n",
    "print(transformed_data)\n",
    "\n",
    "print('Other data transformed by the same rescaler: ')\n",
    "#We can also transform data other than the ones used to fit the scaler\n",
    "print(max_abs_scaler.transform([[2,2,2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling to have a mean of 0 and standard deviation of 1\n",
    "\n",
    "For many algorithms, problems arise if a feature has a variance that is significantly larger than the variance of other features. It might dominate the objective function and make the estimator unable to \"learn\" properly. To solve this problem, we can transform our data using the `scale` function provided by the `sklearn.preprocessing` package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[ 1., -1.,  2.], [ 2.,  0.,  0.],[ 0.,  1., -1.]])\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print('The scaled data: ')\n",
    "print(scaled_data)\n",
    "print(scaled_data.mean(axis=0))\n",
    "print(scaled_data.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn!\n",
    "Scaling data can have an effect on the quality of our trained models. To illustrate this, take the *wine dataset* provided by sklearn datasets and train three different logistic regression models predicting the target. Compare their accuracy scores.\n",
    "\n",
    "* Model 1 should be trained on the original data set.\n",
    "* Model 2 should be trained on a data set scaled using the MinMaxScaler with range [0,1].\n",
    "* Model 3 should be trained on a data set scaled to the attributes having a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretization / Binning\n",
    "Discretization provides a way to partition continuous features into discrete values. Various algorithms and approaches exist to achieve this. \n",
    "\n",
    "Here, we introduce the KBinsDiscretizer, which discretizes the features into bins according to several paramters. \n",
    "\n",
    "By default, each feature is split into 5 bins. This can be configured with *n_bins* parameter. The parameter can be set using an integer (used for all features) or an array (different values for each feature possible).\n",
    "\n",
    "By default, the output is one-hot encoded. This can be configured with the *encode* parameter:\n",
    "\n",
    "* Onehot: Encodes the results with one-hot encoding and returns a sparse matrix.\n",
    "* Onehot-dense: Encodes the results with one-hot encoding and returns a dense array.\n",
    "* Ordinal: returns the bin identifier encoded as an integer value.\n",
    "\n",
    "By default, the data is split into bins with equal number of data points. This can be configured with the *strategy* parameter:\n",
    "\n",
    "* Uniform: All bins in each feature have identical width.\n",
    "* Quantile: All bins in each feature have the same number of points.\n",
    "* kmeans: Values in each bin have the same nearest center of 1D k-means cluster.\n",
    "\n",
    "Below you can find example code for applying the KBinsDiscretizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[ -3., 5., 15 ],[  0., 6., 14 ],[  6., 3., 11 ]])\n",
    "discretizer = preprocessing.KBinsDiscretizer(n_bins=[3,2,2], encode='ordinal', strategy = 'uniform')\n",
    "discretizer.fit(data)\n",
    "discretized_data = discretizer.transform(data)\n",
    "print('The discretized data:')\n",
    "print(discretized_data)\n",
    "\n",
    "print('The edges of each bin per feature:')\n",
    "#displaying the edges of each bin per feature\n",
    "print(discretizer.bin_edges_[0])\n",
    "print(discretizer.bin_edges_[1])\n",
    "print(discretizer.bin_edges_[2])\n",
    "\n",
    "print('The number of bins per feature:')\n",
    "#displaying the number of bins per feature (if bins are too small they can be deleted with a warning)\n",
    "print(discretizer.n_bins_[0])\n",
    "print(discretizer.n_bins_[1])\n",
    "print(discretizer.n_bins_[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn!\n",
    "Apply the KBinsDiscretizer on the data given below. The number of bins for each feature should be equal to the range of that feature. Use the encoding *onehot-dense* and choose the strategy such that all bins contains the same number of data points.\n",
    "Print the discretized data as well as the bin boundaries for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[-2, 1, -4,   -1],\n",
    "                    [-1, 2, -3, -0.5],\n",
    "                    [ 0, 3, -2,  0.5],\n",
    "                    [ 1, 4, -1,    2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating Data\n",
    "Let's have a look at how we can combine `DataFrame`s that have a common index. \n",
    "`Pandas` makes that very easy providing the `df.join` function.\n",
    "*(If you want to use this function in your post-ids life, i.e., not in the assignment, check the types of joins first - left join, right join, outer join)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create wome random data sampled from normal distributions.\n",
    "df1 = pd.DataFrame(np.random.normal(loc=0, scale=1, size=(100, 2)), columns=['x', 'y'], index=np.arange(100))\n",
    "df2 = pd.DataFrame(np.random.normal(loc=2, scale=2, size=(100, 2)), columns=['x', 'y'], index=np.arange(100, 200))\n",
    "df_data = pd.concat([df1, df2], axis=0)\n",
    "df_classes = pd.DataFrame(np.concatenate([np.full((100, 1), 'c1'), np.full((100, 1), 'c2')], axis=0), columns=['label'], index=np.arange(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join on the index (left-outer join)\n",
    "df_merged = df_data.join(df_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "sns.scatterplot(x='x', y='y', hue='label', data=df_merged, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced visualizations\n",
    "Recall basic visualization techniques from instruction session 2, such as box plots, bar charts etc. Here, we will work with more advanced techniques.\n",
    "\n",
    "The following section shows an extract of the multitude of visualization possibilities. Python provides the advantage that many plots can be created with a few simple lines of code. Often, tuning the layout is more difficult than the visualization itself.\n",
    "\n",
    "In the following, we will show some vizualizations as introduced in the lecture.\n",
    "\n",
    "## Scatter Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# we use the iris dataset\n",
    "import seaborn as sns\n",
    "data = sns.load_dataset('iris')\n",
    "\n",
    "# Make the plot\n",
    "scatter_matrix(data, alpha=0.2, figsize=(6, 6), diagonal='kde')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Coordinate plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import parallel_coordinates\n",
    "# we use the iris dataset again\n",
    "\n",
    "# Make the plot\n",
    "parallel_coordinates(data, 'species', colormap=plt.get_cmap(\"Set2\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "data['species_id'] = data['species'].replace(data['species'].unique(), [1, 2, 3])\n",
    "fig = px.parallel_coordinates(data, \n",
    "                              color=\"species_id\",\n",
    "                              labels={\"species_id\": \"Species\",\n",
    "                                    \"sepal_width\": \"Sepal Width\", \"sepal_length\": \"Sepal Length\",\n",
    "                                    \"petal_width\": \"Petal Width\", \"petal_length\": \"Petal Length\", },\n",
    "                              # color_continuous_scale=px.colors.diverging.Tealrose,\n",
    "                              # color_continuous_midpoint=2\n",
    "                             )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamgraph\n",
    "To visualize a streamgraph in Python we can make use of stackplots from the *matplotlib* package. Stackplots are generated by plotting different datasets vertically on top of one another rather than overlapping with one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the values for our x-axis\n",
    "x = [1, 2, 3, 4, 5]\n",
    "# the values that will be stacked on top of each other\n",
    "y1 = [1, 1, 2, 3, 5]\n",
    "y2 = [0, 4, 2, 6, 8]\n",
    "y3 = [1, 3, 5, 7, 9]\n",
    "\n",
    "# the labels for y1, y2 and y3\n",
    "labels = [\"Fibonacci \", \"Evens\", \"Odds\"]\n",
    "\n",
    "#stacking our values vertically\n",
    "y = np.vstack([y1, y2, y3])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "#modifying the axis\n",
    "ax.stackplot(x, y1, y2, y3, labels=labels, baseline='wiggle')\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap\n",
    "We use the heatmap function from the *seaborn package* to create a heatmap. Note, we have to aggregate/pivot our data into the correct shape before the heatmap can be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the example flights dataset\n",
    "# The example flights dataset is in long-format.\n",
    "flights_long = sns.load_dataset(\"flights\")\n",
    "#In long-format, each data row can be interpreted individually. In this example each row contains a value for *year*, *month* \n",
    "#and *passengers*.\n",
    "print('Long-format:')\n",
    "print(flights_long) \n",
    "print(' ')\n",
    "\n",
    "# We have to convert the data to wide-format to be able to use it for the heatmap.\n",
    "#In wide-form, the values correspond to a combination of the *row category* and the *column category*. \n",
    "#When transforming from long-format to wide-format, categorical data will be grouped. In this example, \n",
    "#the wide-format displays the number of passangers for all combinations of *month* and *year*.\n",
    "# Since there is only one value per month and year, we do not have aggregate our data in this example.\n",
    "print('Wide-format:')\n",
    "flights_wide = flights_long.pivot(\"month\", \"year\", \"passengers\")\n",
    "print(flights_wide)\n",
    "\n",
    "# Draw a heatmap with the numeric values in each cell\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "sns.heatmap(flights_wide, annot=True, fmt=\"d\", linewidths=.5, ax=ax)\n",
    "# For longer ticks it is sometimes helpful to rotate them\n",
    "ax.tick_params(axis='x', labelrotation=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn!\n",
    "\n",
    "Read the following json data about global temperature monthly variance and plot it on a heatmap, where\n",
    "- the x-axis is the year \n",
    "- the y-axis is the month\n",
    "- cells are the variance\n",
    "\n",
    "You can use the seaborn function introduced above or the imshow function from plotly.express to make the heatmap more interactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('https://raw.githubusercontent.com/freeCodeCamp/ProjectReferenceData/master/global-temperature.json')\n",
    "print('baseTemperature:', df.baseTemperature[0])\n",
    "df = pd.DataFrame(df['monthlyVariance'].tolist())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sankey\n",
    "Even though `matplotlib` already has the functionality to create Sankey diagrams, the Sankey charts produced by `plotly` are admittedly much prettier.\n",
    "Creating a basic Sankey chart using `plotly` is straightforward although it requires quite some boilerplate code.\n",
    "In essence, you have to specify flows by means of and edge list and a list of flow values. Besides, you have to provide a list of vertex labels (label at index i is the label for vertex i). Finally, you can also provide lists of colors for the vertices and links. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following artificial dataset where we have collected data on students who participated in an exam. \n",
    "Each row comprises the student who attempted the exam; whether it is his first, second, or third attempt; and the achieved grade.\n",
    "After aggregating the data, we obtain the following Sankey diagram.\n",
    "\n",
    "Evaluate the following visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Data edge list and corresponding values\n",
    "l_source = [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "l_target = [3, 4, 5, 3, 4, 5, 3, 4, 5]\n",
    "l_labels = ['Attempt 1', 'Attempt 2', 'Attempt 3', 'Grade [1,3)', 'Grade [3,4)', 'Grade 5']\n",
    "l_values = [200, 400, 400, 100, 200, 100, 25, 100, 20] \n",
    "\n",
    "# Define Sankes diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node = dict(\n",
    "      pad = 15,\n",
    "      thickness = 20,\n",
    "      line = dict(color = \"black\", width = 0.5),\n",
    "      label = l_labels,\n",
    "      color = \"blue\"\n",
    "    ),\n",
    "    link = dict(\n",
    "      source = l_source,\n",
    "      target = l_target,\n",
    "      value = l_values\n",
    "  ))])\n",
    "\n",
    "fig.update_layout(title_text=\"Basic Sankey Diagram\", font_size=10)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn!\n",
    "In real-life no one is going to tell you which visualization technique to use for which data! \n",
    "Pretend that you are a data scientist who was just provided with the datasets used in this instruction session. Visualize the data using the previously presented techniques and/or other suitable techniques provided by the matplotlib (https://matplotlib.org/gallery/index.html) and the seaborn packages (https://seaborn.pydata.org/examples/index.html). Decide by yourself which part of the data you want to visualize using which plot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
